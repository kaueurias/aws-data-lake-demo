# ğŸ—ï¸ AWS Data Lake Demo - Terraform Infrastructure

[![Terraform](https://img.shields.io/badge/Terraform-1.0+-623CE4?logo=terraform&logoColor=white)](https://terraform.io)
[![AWS](https://img.shields.io/badge/AWS-Cloud-FF9900?logo=amazon-aws&logoColor=white)](https://aws.amazon.com)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Infraestrutura completa de Data Lake na AWS usando Terraform, com pipeline de ingestÃ£o em tempo real, processamento ETL e visualizaÃ§Ã£o de dados.**

## ğŸŒ Languages / Idiomas

- ğŸ‡ºğŸ‡¸ **English**: You're reading it!
- ğŸ‡§ğŸ‡· **PortuguÃªs**: [Guia completo de implantaÃ§Ã£o](GUIA_IMPLANTACAO.md)

## ğŸ“‹ Overview

This project demonstrates a complete AWS Data Lake implementation, including:

- **Real-time ingestion** via Kinesis Data Firehose
- **Automated ETL processing** with AWS Glue
- **Optimized storage** in S3 (raw + processed)
- **SQL queries** with Amazon Athena
- **Integrated monitoring** with CloudWatch
- **Data visualization** with Amazon QuickSight

## ğŸ—ï¸ Arquitetura

```mermaid
graph LR
    A[KDG] --> B[Kinesis Firehose]
    B --> C[S3 Raw Data]
    C --> D[Lambda Trigger]
    D --> E[Glue ETL Job]
    E --> F[S3 Processed Data]
    F --> G[Athena]
    G --> H[QuickSight]
    
    I[CloudWatch] --> B
    I --> D
    I --> E
```

### Componentes

| Service | Function | Configuration |
|---------|----------|---------------|
| **Kinesis Firehose** | IngestÃ£o de dados | Buffer: 1MB/1min |
| **S3** | Data Lake Storage | Raw + Processed |
| **Lambda** | TransformaÃ§Ã£o + Trigger | Python 3.9 |
| **Glue** | ETL Processing | Spark, Parquet |
| **Athena** | SQL Queries | Workgroup dedicado |
| **CloudWatch** | Monitoring | 7 dias retenÃ§Ã£o |

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install dependencies
terraform --version  # >= 1.0
aws --version        # >= 2.0
```

### Deployment

```bash
# 1. Clone repository
git clone https://github.com/your-username/aws-data-lake-demo.git
cd aws-data-lake-demo

# 2. Configure AWS
aws configure

# 3. Deploy infrastructure
terraform init
terraform plan
terraform apply
```

### KDG Setup

1. **CloudFormation**: [Create Cognito User](https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html)
2. **Template KDG**:
```json
{
  "year": "{{random.number({\"min\":1850,\"max\":1900})}}",
  "firstname": "{{name.firstName}}",
  "lastname": "{{name.lastName}}",
  "city": "{{address.city}}",
  "state": "{{address.state}}",
  "transactionamount": {{random.number({"min":10,"max":150})}}
}
```

## ğŸ“Š Resources Created

### Core Infrastructure
- âœ… S3 Bucket (versionado)
- âœ… Kinesis Firehose Delivery Stream
- âœ… Lambda Functions (2x)
- âœ… Glue Database + Job + Table
- âœ… Athena Workgroup
- âœ… IAM Roles + Policies

### Monitoramento
- âœ… CloudWatch Log Groups
- âœ… Firehose Logging
- âœ… Lambda Error Tracking
- âœ… Glue Job Monitoring

## ğŸ§ª Testing

### Send Data
```bash
# Via KDG (interface web)
# Ou via AWS CLI
aws firehose put-record \
  --delivery-stream-name data-lake-demo-firehose \
  --record Data='{"name":"test","value":123}'
```

### Query Data
```sql
-- Athena Console
SELECT COUNT(*) FROM processed_data;

SELECT city, COUNT(*) as total 
FROM processed_data 
GROUP BY city 
ORDER BY total DESC 
LIMIT 10;
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.tf                 # Main resources
â”œâ”€â”€ iam.tf                  # Roles and policies
â”œâ”€â”€ glue_catalog.tf         # Tables and schema
â”œâ”€â”€ cloudwatch.tf           # Logs and monitoring
â”œâ”€â”€ outputs.tf              # Terraform outputs
â”œâ”€â”€ kdg_template.json       # KDG template
â”œâ”€â”€ GUIA_IMPLANTACAO.md     # Complete guide (PT-BR)
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Advanced Configuration

### Terraform Variables

```hcl
variable "aws_region" {
  description = "AWS region"
  default     = "us-east-1"
}

variable "project_name" {
  description = "Project name prefix"
  default     = "data-lake-demo"
}
```

### Customization

- **Firehose Buffer**: Adjust `buffering_size` and `buffering_interval`
- **Glue Workers**: Modify `number_of_workers` based on volume
- **Log Retention**: Change `retention_in_days` in CloudWatch

## ğŸ“Š Monitoramento

### CloudWatch Dashboards

Acesse os logs em:
- `/aws/kinesisfirehose/data-lake-demo-firehose`
- `/aws/lambda/data-lake-demo-transformer`
- `/aws/lambda/data-lake-demo-glue-trigger`
- `/aws-glue/jobs/logs-v2`

### Important Metrics

- **Firehose**: DeliveryToS3.Records
- **Lambda**: Duration, Errors
- **Glue**: Job Success Rate
- **S3**: Object Count, Storage

## ğŸ”’ Security

### IAM Policies
- **Least privilege principle**
- **Service-specific roles**
- **No hardcoded credentials**

### Data
- **Encryption at rest** (S3)
- **Encryption in transit** (HTTPS)
- **VPC endpoints** (optional)

## ğŸ’° Estimated Costs

| Service | Monthly Cost (estimated) |
|---------|-------------------------|
| S3 | $5-20 |
| Firehose | $10-30 |
| Lambda | $1-5 |
| Glue | $10-50 |
| Athena | $5-15 |
| **Total** | **$31-120** |

> *Costs vary based on data volume and region*

## ğŸ§¹ Cleanup

```bash
# Empty S3 bucket (AWS console)
# Then run:
terraform destroy
```

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Open a Pull Request

## ğŸ“ Roadmap

- [ ] IntegraÃ§Ã£o com AWS Step Functions
- [ ] Suporte a mÃºltiplas regiÃµes
- [ ] Terraform modules
- [ ] CI/CD pipeline
- [ ] Testes automatizados

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™‹â€â™‚ï¸ Support

- **Issues**: [GitHub Issues](https://github.com/your-username/aws-data-lake-demo/issues)
- **Documentation**: [Complete Guide (PT-BR)](GUIA_IMPLANTACAO.md)
- **AWS Docs**: [Data Lake Guide](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/welcome.html)

---

**â­ If this project was helpful, please give it a star!**

**Built with â¤ï¸ for the AWS community**